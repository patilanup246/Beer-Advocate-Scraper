course_data[nrow(course_data)+1,] <- scrape_html(course_urls)
course_urls[1]
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[])
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[1])
readLines(course_urls[1])
readLines(course_urls[1])
library(XML)
library(XML)
getURL(course_urls[1])
library(RCurl)
library(RCurl)
getURL(course_urls[1])
readLines(getURL(course_urls[1]))
readLines(getURL(course_urls[1]))
readLines(getURL(course_urls[1]))
readLines(course_urls[1])
course_urls[1]
readLines(paste("http://", "www.sfu.ca/outlines.html?2018/spring/arch/100/c100"))
readLines(paste("https://", "www.sfu.ca/outlines.html?2018/spring/arch/100/c100"))
readLines("https://www.sfu.ca/outlines.html?2018/spring/arch/100/c100")
course_urls <- paste("https://", course_urls)
course_urls <- paste("https://",course_urls)
course_urls
course_urls <- readLines("http://code.sfu.ca/undergrad/course-outlines/course-outlines-1181.html")
course_urls <- course_urls[grep(".*outlines\\.html.*", course_urls)]
course_urls <- gsub(".*(www\\.sfu.*[a-z][0-9][0-9][0-9]).*", "\\1", course_urls)
course_urls <- course_urls[grep(".*(www\\.sfu.*[a-z][0-9][0-9][0-9]).*", course_urls)]
#add "HTTPS"
course_urls <- paste("https://",course_urls)
course_urls
#remove blank
course_urls <- gsub("[[:blank:]]", '', course_urls)
course_urls
course_data[nrow(course_data)+1,] <- scrape_html(course_urls)
#Read html and store them into data frame "course_data"
scrape_html <- function(html){
course_page=suppressWarnings(readLines(html))
number <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\2", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
term <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\1", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
title <- gsub("[[:blank:]]*(.*)","\\1", course_page[grep("<h2 id=\"title\">", course_page)+1])
instructor <- gsub(".*>(.*)<.*","\\1", course_page[grep("<h4>Instructor", course_page)+1])
instructor <- gsub("[[:blank:]]*(.*)", "\\1", instructor)
time <- gsub("[[:blank:]]*<p>(.*)<br>(.*)<\\/p>","\\1", course_page[grep("Course Times", course_page)+1])
time <- gsub("&ndash;", '-', time)
location <- gsub("[[:blank:]]*<p>(.*)<br>(.*)<\\/p>","\\2", course_page[grep("Course Times", course_page)+1])
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
reading <- gsub(".*<p>(.*)<br>(.*)<\\/p>", "\\1", reading)
reading <- gsub(".*<strong>(.*)<\\/strong>.*", "\\1", reading)
reading <- gsub("<.*>", "", reading)
reading <- gsub(",[[:blank:]][[:blank:]]+", "", reading)
#new_row <- data.frame(url=html, number=number, title=title, instructor=instructor, term=term, time=time, location=location, textbook=reading)
#course_data <- merge(course_data, new_row, by="url", all = T)
#cbind(course_data, list(html, number, title, instructor, term, time, location, reading))
#course_data[nrow(course_data)+1, url]=html
#course_data[nrow(course_data)+1,] <- c(html, number, title, instructor, term, time, location, reading)
return(c(html, number, term, title, instructor, time, location, reading))
}
course_data[nrow(course_data)+1,] <- scrape_html(course_urls)
course_urls
readLines(course_urls[1])
course_data[nrow(course_data)+1,] <- scrape_html(course_urls)
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[1])
?tryCatch()
#Read html and store them into data frame "course_data"
scrape_html <- function(html){
tryCatch(course_page=suppressWarnings(readLines(html))  )
number <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\2", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
term <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\1", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
tryCatch(  reading_block <- grep("READING", course_page))
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
reading <- gsub(".*<p>(.*)<br>(.*)<\\/p>", "\\1", reading)
reading <- gsub(".*<strong>(.*)<\\/strong>.*", "\\1", reading)
reading <- gsub("<.*>", "", reading)
reading <- gsub(",[[:blank:]][[:blank:]]+", "", reading)
return(c(html, number, term, title, instructor, time, location, reading))
}
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[1])
tryCatch(expr=course_page=suppressWarnings(readLines(html))  )
#initialize data frame
course_data <- data.frame(url=character(), number=character(), term=character(),
textbook=character(), stringsAsFactors = FALSE)
#Read html and store them into data frame "course_data"
scrape_html <- function(html){
course_page=suppressWarnings(readLines(html))
number <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\2", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
term <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\1", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
reading <- gsub(".*<p>(.*)<br>(.*)<\\/p>", "\\1", reading)
reading <- gsub(".*<strong>(.*)<\\/strong>.*", "\\1", reading)
reading <- gsub("<.*>", "", reading)
reading <- gsub(",[[:blank:]][[:blank:]]+", "", reading)
return(c(html, number, term, title, instructor, time, location, reading))
}
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[1])
return(c(html, number, term, reading))
return(c(html, number, term, reading))
#Read html and store them into data frame "course_data"
scrape_html <- function(html){
course_page=suppressWarnings(readLines(html))
number <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\2", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
term <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\1", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
reading <- gsub(".*<p>(.*)<br>(.*)<\\/p>", "\\1", reading)
reading <- gsub(".*<strong>(.*)<\\/strong>.*", "\\1", reading)
reading <- gsub("<.*>", "", reading)
reading <- gsub(",[[:blank:]][[:blank:]]+", "", reading)
return(c(html, number, term, reading))
}
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[1])
View(course_data)
course_data[nrow(course_data)+1,] <- scrape_html(course_urls)
course_data[nrow(course_data)+1,] <- scrape_html(course_urls[2])
View(course_data)
length(course_urls)
for(i in length(course_urls)){
try(course_data[nrow(course_data)+1,] <- scrape_html(course_urls[i]), silent=TRUE)
}
View(course_data)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
course_page=suppressWarnings(readLines(html))
course_page=suppressWarnings(readLines(course_urls[1]))
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading
reading <- toString(reading)
reading
reading <- gsub(".*<p>(.*)<br>(.*)<\\/p>", "\\1", reading)
reading
reading <- gsub(".*<strong>(.*)<\\/strong>.*", "\\1", reading)
reading
reading <- gsub("<.*>", "", reading)
reading
reading <- gsub(",[[:blank:]][[:blank:]]+", "", reading)
reading
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
reading
gsub("<.*>", "", reading)
gsub("(<.*>)", "", reading)
reading <- toString(reading)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
gsub("(<.*>)", "", reading)
reading
gsub("(<.*\\b>)", "", reading)
gsub(".*(<.*>).*", "", reading)
gsub(",", "", reading)
gsub("<.*>", "", reading)
gsub("<[^>]*>", "", reading)
gsub("<[^>]*>", "", reading)
gsub("<[^>]*>", "", reading)
gsub("<[^>]*>", "", reading)
gsub("<[^>]*>", "", reading)
reading <- gsub("<[^>]*>", "", reading)
reading
gsub(",", '', reading)
reading <- gsub(",", '', reading)
gsub("[[:blank:]]{2,}",'', reading)
#remove "required Reading
reading <- gsub("REQUIRED READING:", "", reading)
reading
#remove "required Reading
reading <- gsub("REQUIRED READING:", '', reading)
reading
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
#remove HTML tags
reading <- gsub("<[^>]*>", "", reading)
#remove commas
reading <- gsub(",", '', reading)
#remove blanks
reading <- gsub("[[:blank:]]{2,}",'', reading)
#remove "required Reading
reading <- gsub("REQUIRED READING:", '', reading)
reading
#remove ISBN
ISBN <- 0+gsub(".*ISBN:[[:blank:]]([0-9]*)","\\1", reading)
#remove ISBN
ISBN <- 0+as.numeric(gsub(".*ISBN:[[:blank:]]([0-9]*)","\\1", reading))
ISBN
#initialize data frame
course_data <- data.frame(url=character(), number=character(), term=character(),
textbook=character(), stringsAsFactors = FALSE)
reading <- gsub("&amp;", "", reading)
reading
gsub("(.)*ISBN:[[:blank:]]([0-9]*)", "\\1", reading)
gsub("(.*)ISBN:[[:blank:]]([0-9]*)", "\\1", reading)
reading <- gsub("(.*)ISBN:[[:blank:]]([0-9]*)", "\\1", reading)
#initialize data frame
course_data <- data.frame(url=character(), number=character(), term=character(),
textbook=character(), ISBN=character(),stringsAsFactors = FALSE)
#Read html and store them into data frame "course_data"
scrape_html <- function(html){
course_page=suppressWarnings(readLines(html))
number <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\2", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
term <- gsub("[[:blank:]]*<h1 id=\\\"name\\\">([A-Za-z0-9]*[[:blank:]][0-9]*)[[:blank:]]*-[[:blank:]]([A-Z]*[[:blank:]][0-9]*).*", "\\1", course_page[grep("<h1 id=\"name\">.*</h1>", course_page)])
reading_block <- grep("READING", course_page)
reading <- course_page[as.numeric(reading_block):as.numeric(reading_block+10)]
reading <- toString(reading)
#remove HTML tags
reading <- gsub("<[^>]*>", "", reading)
#remove commas
reading <- gsub(",", '', reading)
#remove blanks
reading <- gsub("[[:blank:]]{2,}",'', reading)
#remove "required Reading
reading <- gsub("REQUIRED READING:", '', reading)
#remove ISBN
ISBN <- gsub(".*ISBN:[[:blank:]]([0-9]*)","\\1", reading)
reading <- gsub("&amp;", "", reading)
reading <- gsub("(.*)ISBN:[[:blank:]]([0-9]*)", "\\1", reading)
return(c(html, number, term, reading, ISBN))
}
for(i in length(course_urls)){
try(course_data[nrow(course_data)+1,] <- scrape_html(course_urls[i]), silent=TRUE)
}
for(length(course_urls)){
try(course_data[nrow(course_data)+1,] <- scrape_html(course_urls[i]), silent=TRUE)
}
for(i in length(course_urls)){
try(course_data[nrow(course_data)+1,] <- scrape_html(course_urls[i]), silent=TRUE)
}
for(i in seq(0:length(course_urls))){
try(course_data[nrow(course_data)+1,] <- scrape_html(course_urls[i]), silent=TRUE)
}
View(course_data)
write.csv(course_data, file="books.csv")
getwd()
con = dbconnect("test.sqlite")
library(DBI)
library(RSQLite)
con = dbConnect("test.sqlite")
con = dbConnect(SQLite(), dbname="test.sqlite")
dbListTables(con)
data <- read.csv("rings_by_hour_in_narnia.csv")
View(data)
weekdays(data[2])
weekdays(data[,2])
data[1, 2]
str(data[1, 2])
as.POSIXct(str(data[1, 2]))
data <- read.csv("rings_by_hour_in_narnia.csv", stringsAsFactors = FALSE)
str(data[1, 2])
as.POSIXct(str(data[1, 2]))
as.POSIXct(str(data[1, 2]), format='%Y-%m-%d %H:%M:%S')
as.POSIXct(data[1, 2], format='%Y-%m-%d %H:%M:%S')
data[1, 2]
as.POSIXct(data[1, 2], format='%Y-%m-%d %H:%M:%S')
as.POSIXct(data[1, 2], format='%Y-%m-%d %H:%M:%S')
data[,2] <- as.POSIXct(data[, 2], format='%Y-%m-%d %H:%M:%S')
data
plot(hour, rings)
plot(data$hour, data$rings)
as.numeric(difftime(data$hour, trunc(data$hour,
"days"), Sys.timezone(), "hours"))
trunc(data$hour, "days")
weekdays(trunc(data$hour, "days"))
hist(weekdays(trunc(data$hour, "days")))
table(weekdays(trunc(data$hour, "days")))
appearDays <- table(weekdays(trunc(data$hour, "days")))
hist(appearDays)
hist(appearDays, breaks=7)
plot(appearDays, breaks=7)
barplot(appearDays)
install.packages("ggplot2")
ggplot2.barplot(appearDays)
ggpbarplot(appearDays)
ggbarplot(appearDays)
geom_bar(appearDays)
ggplot(appearDays)
?ggplot
??ggplot
ggplot(appearDays, ase(fl))
ggplot2(appearDays, ase(fl))
library(ggplot2)
ggplot(appearDays, ase(fl))
ggplot(appearDays)
ggplot(, aes(appearDays))
ggplot() aes(appearDays))
ggplot( aes(appearDays))
barplot(appearDays)
days
as.numeric(difftime(data$hour, trunc(data$hour,
"days"), Sys.timezone(), "hours"))
as.numeric(difftime(data$hour, trunc(data$hour,
"weeks"), Sys.timezone(), "hours"))
nrows(data)
nrow(data)
as.numeric(difftime(data$hour, trunc(data$hour,
"weeks"), Sys.timezone(), "hours"))
as.numeric(difftime(data$hour, trunc(data$hour,
"days"), Sys.timezone(), "hours"))
data[, 3] <- as.numeric(difftime(data$hour, trunc(data$hour,
"days"), Sys.timezone(), "hours"))
table(data[, c(1,3)])
data[, 3] <- as.numeric(difftime(data$hour, trunc(data$hour,
"days"), Sys.timezone(), "hours"))
data[,3]
hist(data[,3], data[,1])
plot(data[,3], data[,1])
scatter.smooth(data[,3], data[,1])
p <- ggplot(data[,3], aes(class, hwy))
p + geom_boxplot()
p <- ggplot(data, aes(class, hwy))
p + geom_boxplot()
p + geom_boxplot()
p <- ggplot(data[,c(1,3)], aes(class, hwy))
p + geom_boxplot()
boxplot(p + geom_boxplot())
boxplot(data[,3], data[,1])
boxplot(data$rings~data$V3)
table(data$V3$month)
data$V3
data$hour$month
data$hour
trunc(data$hour, "month")
month(data$hour)
months(data$hour)
table(months(data$hour))
seq(table(months(data$hour)))
scatter.smooth(data[,3], data[,1], col=seq(table(months(data$hour))))
which(table(months(data$hour)), months(data$V3))
months(data$V3)
which(table(months(data$hour)), months(data$hour))
?which
months(data$hour)
table(months(data$hour))
table(months(data$hour))["February"]
table(months(data$hour))=seq(table(months(data$hour)))
table(months(data$hour))$value=seq(table(months(data$hour)))
str(table(months(data$hour)))
str(table(months(data$hour)))
table(months(data$hour))
unique(months(data$hour))
unique(months(data$hour))["February"]
unique(months(data$hour))["March"]
which(unique(months(data$hour))=="March")
scatter.smooth(data[,3], data[,1], col=which(unique(months(data$hour))==months(data$hour)))
which(unique(months(data$hour))==months(data[166,2])
)
which(unique(months(data$hour))==months(data[1000,2]))
scatter.smooth(data[,3], data[,1], col=as.integer((which(unique(months(data$hour))==months(data$hour))-1)/3))
which(unique(months(data$hour))==months(data$hour
)
)
which(unique(months(data$hour))==months(data$hour))
which(months(data$hour)==unique(months(data$hour)))
which(unique(months(data$hour)))
which(unique(months(data$hour))==months(data$hour))
unique(months(data$hour))
months <- unique(months(data$hour))
which(months==months(data$hour)
_
)
which(months==months(data$hour))
which(months(data$hour)==months)
scatter.smooth(data[,3], data[,1], col=as.integer((which(data_months==months(data$hour))-1)/3))
data_months <- unique(months(data$hour))
which(data_months==months(data$hour))
match(months(data$hour), data_months)
scatter.smooth(data[,3], data[,1], col=as.integer((match(months(data$hour), data_months)-1)/3))
scatter.smooth(data[,3], data[,1], col=as.integer((match(months(data$hour), data_months))/3))
trunc(elonDF$created,"days"), tz="America/Los_angeles", "hours")
library(twitteR)
library(ROAuth)
consumer_key = "IpcxWsgEM7YNCiSxf8llZjjIe"
consumer_secret = "pQ7WIbFQBP2vtmtYYM4J8DvPNQA47egcpa2Un7V88xj0LK8V3m"
access_token = "119604012-8HLT3SsumkjTn0Nxchr97btN34dYJWRX82KIKY94"
access_secret = "s1AWjBohYtBVJ3tIVXCqkP40fV7OZcpeYtxXM0D8yvqtf"
requestURL = "https://api.twitter.com/oauth/request_token"
accessURL ="https://api.twitter.com/oauth/access_token"
authURL ="https://api.twitter.com/oauth/authorize"
my_oauth = OAuthFactory$new(consumerKey=consumer_key, # still need your key
consumerSecret=consumer_secret, # still need your secret
requestURL=requestURL, accessURL=accessURL, authURL=authURL)
my_oauth$handshake() # Send R to requested site to authenticate
my_oauth = OAuthFactory$new(consumerKey=consumer_key, # still need your key
consumerSecret=consumer_secret, # still need your secret
requestURL=requestURL, accessURL=accessURL, authURL=authURL)
library(twitteR)
install.packages("twitteR")
install.packages("ROAuth")
library(twitteR)
library(ROAuth)
consumer_key = "IpcxWsgEM7YNCiSxf8llZjjIe"
consumer_secret = "pQ7WIbFQBP2vtmtYYM4J8DvPNQA47egcpa2Un7V88xj0LK8V3m"
access_token = "119604012-8HLT3SsumkjTn0Nxchr97btN34dYJWRX82KIKY94"
access_secret = "s1AWjBohYtBVJ3tIVXCqkP40fV7OZcpeYtxXM0D8yvqtf"
requestURL = "https://api.twitter.com/oauth/request_token"
accessURL ="https://api.twitter.com/oauth/access_token"
authURL ="https://api.twitter.com/oauth/authorize"
my_oauth = OAuthFactory$new(consumerKey=consumer_key, # still need your key
consumerSecret=consumer_secret, # still need your secret
requestURL=requestURL, accessURL=accessURL, authURL=authURL)
my_oauth$handshake() # Send R to requested site to authenticate
setup_twitter_oauth(consumer_key, consumer_secret,
access_token, access_secret)
elonTweets <- userTimeline("elonmusk", n=3200, includeRts = TRUE)
elonDF <- twListToDF(elonTweets)
tweetTime <- as.numeric(difftime(elonDF$created,
)
;
trunc(elonDF$created,"days"), tz="America/Los_angeles", "hours")
tweetTime <- as.numeric(difftime(elonDF$created, trunc(elonDF$created,"days"), tz="America/Los_angeles", "hours")
)
tweetTime <- as.numeric(difftime(elonDF$created,
trunc(elonDF$created,"days"), tz="America/Los_angeles", "hours")
)
#q4
hist(nchar(elonDF$text), main="Ditribution of characters in Elon Musk's tweets", xlab= "Tweet length", breaks = 50)
update90
update()
install.packages(c("htmlwidgets", "httpuv", "leaflet", "pillar", "sourcetools", "stringi", "yaml"))
info = read.csv(file = "C:\Users\Zongqi Wang\Documents\comments\beerinfo.csv")
info = read.csv(file = "C:/Users/Zongqi Wang/Documents/comments/beerinfo.csv")
info
head(info)
info$brewery_number="profile"
head(info)
info = read.csv(file = "C:/Users/Zongqi Wang/Documents/comments/beerinfo.csv")
which(info$brewery_number=="profile")
info[which(info$brewery_number=="profile"), "brewery_number"]
info = info[-which(info$brewery_number=="profile")]
info
dim(info)
head(info)
#reading csv
info = read.csv(file = "C:/Users/Zongqi Wang/Documents/comments/beerinfo.csv")
#removing rows that are not breweries
info_noempty = info[-which(info$brewery_number=="profile")]
dim(info)
dim(info_noempty)
head(info_noempty)
#removing rows that are not breweries
info_noempty = info[-which(info$brewery_number=="profile"),]
dim(info_noempty)
View(info_noempty)
info_noempty[1:10, "availability"]
info[which(info$brewery_number=="profile"),"brewery_number"]
beer_numbers = info[which(info$brewery_number=="profile"),"beer_number"]
beer_numbers
comments = read.csv(file = "C:/Users/Zongqi Wang/Documents/comments/comment.csv")
getwd()
setwd("./Beer-Advocate-Scraper/")
ls
ls()
comments = read.csv("comment.csv")
brewerys = read.csv("breweryinfo.csv")
beers = read.csv("beerinfo.csv")
dim(comments)
dim(brewerys)
install.packages("RTextTools")
install.packages("topicmodels")
vignette(”topicmodels”)
vignette(topicmodels)
vignette("topicmodels")
vignette(”topicmodels”)
install.packages("tm")
install.packages("Snowballc")
install.packages("SnowballC")
library(RTextTools)
library(topicmodels)
library(tm)
library(SnowballC)
comments_text <- comments[250:2500, "comment"]
myCorpus = Corpus(VectorSource(comments_text))
myCorpus
inspect(myCorpus[1:3])
comments_text
(myCorpus = Corpus(VectorSource(comments_text)))
vignette("tm")
gsub("<br>", "", comments_text)
lp = 1
while (lp <= length(myCorpus)) {
if (nchar(as.character(myCorpus[[lp]])) > 100) {
lp = lp + 1
} else {
myCorpus = myCorpus[-lp]
}
}
while (lp <= length(myCorpus)) {
if (nchar(as.character(myCorpus[[lp]])) > 100) {
lp = lp + 1
} else {
myCorpus = myCorpus[-lp]
}
}
installed.packages("foreach")
library("foreach")
library(foreach)
(myCorpus = Corpus(VectorSource(comments_text)))
comments_text <- comments[250:300, "comment"]
(myCorpus = Corpus(VectorSource(comments_text)))
comments_text
str(comments_text)
str(comments_text[1])
comments_text <- as.string(comments[250:300, "comment"])
comments_text <- as.String(comments[250:300, "comment"])
str(comments_text)
(myCorpus = Corpus(VectorSource(comments_text)))
inspect(myCorpus[1:3])
inspect(myCorpus[1:3])
